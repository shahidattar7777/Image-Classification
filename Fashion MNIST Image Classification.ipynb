{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "neZ1sBfAscH0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import random\n",
    "import struct\n",
    "import torch\n",
    "import errno\n",
    "import math\n",
    "import gzip\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DSOr4QzPstRI"
   },
   "outputs": [],
   "source": [
    "class fmnist(Dataset):\n",
    "#Referred from https://github.com/zalandoresearch/fashion-mnist\n",
    "#Referred from https://github.com/ashmeet13/FashionMNIST-CNN/tree/master\n",
    "    urls = ['http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
    "    'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz', 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
    "    'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    file_name =['train-images-idx3-ubyte', 'train-labels-idx1-ubyte', 't10k-images-idx3-ubyte', 't10k-labels-idx1-ubyte']\n",
    "\n",
    "    raw=\"raw\"\n",
    "    processed=\"processed\"\n",
    "\n",
    "\n",
    "    def __init__(self, root, train=True, transform=True, download=True):\n",
    "        super(fmnist, self).__init__()\n",
    "        self.root = root\n",
    "        self.train=train\n",
    "        self.transform=transform\n",
    "        #transform and standardize the image to tensor\n",
    "        self.tensor_transform = transforms.ToTensor()\n",
    "        raw_path = os.path.join(self.root,self.raw)\n",
    "\n",
    "        if download and (os.path.exists(raw_path) == False):\n",
    "            self.download(self.root)\n",
    "\n",
    "        if self.train:\n",
    "            train_path = os.path.join(self.root,self.processed,\"training_set.pt\")\n",
    "            self.train_images, self.train_labels = torch.load(train_path)\n",
    "        else:\n",
    "            test_path = os.path.join(self.root,self.processed,\"testing_set.pt\")\n",
    "            self.test_images, self.test_labels = torch.load(test_path)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if self.train:\n",
    "            image, label = self.train_images[index], self.train_labels[index]\n",
    "        else:\n",
    "            image, label = self.test_images[index], self.test_labels[index]\n",
    "\n",
    "        image = image.numpy()\n",
    "        image = np.rot90(image,axes = (1,2)).copy()\n",
    "\n",
    "        if self.transform and self.train:\n",
    "            image = self.transform_process(image)\n",
    "\n",
    "        image = self.tensor_transform(image)\n",
    "        image = image.contiguous()\n",
    "        image = image.view(1,28,28)\n",
    "        return image,label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return(len(self.train_images))\n",
    "        else:\n",
    "            return(len(self.test_images))\n",
    "\n",
    "\n",
    "    def transform_process(self, image): #Would or would not return a flipped image\n",
    "        self.rotate = random.getrandbits(1)\n",
    "        image = np.flip(image,self.rotate).copy()\n",
    "        return image\n",
    "\n",
    "    '''\n",
    "    download(root) -> The function will download and save the MNIST images in raw\n",
    "    format under the 'raw' folder under the user specified root directory\n",
    "    '''\n",
    "\n",
    "    def download(self, root):\n",
    "        raw_path = os.path.join(self.root,self.raw)\n",
    "        processed_path = os.path.join(self.root,self.processed)\n",
    "\n",
    "        try:\n",
    "            os.makedirs(raw_path)\n",
    "            os.makedirs(processed_path)\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "            pass\n",
    "\n",
    "        for file_index in range(len(self.file_name)):\n",
    "            print(\"Downloading:\",self.urls[file_index])\n",
    "            urllib.request.urlretrieve(self.urls[file_index],(self.file_name[file_index]+'.gz'))\n",
    "            print(\"Extracting:\",self.file_name[file_index]+\".gz\")\n",
    "            f = gzip.open(self.file_name[file_index]+'.gz', 'rb')\n",
    "            with open(raw_path+\"/\"+self.file_name[file_index],'wb') as w:\n",
    "                for line in f.readlines():\n",
    "                    w.write(line)\n",
    "            f.close()\n",
    "            os.remove(self.file_name[file_index]+\".gz\")\n",
    "\n",
    "        print()\n",
    "        print(\"Raw data downloaded and extracted in your specified root directory under /raw\")\n",
    "        print()\n",
    "        self.process(self.root)\n",
    "\n",
    "    '''\n",
    "    process(root) -> Will process the raw downloaded files into a usable format\n",
    "    and store them into the a 'processed' folder under user specified root\n",
    "    directory.\n",
    "    '''\n",
    "\n",
    "    def process(self, root):\n",
    "        raw_path = os.path.join(self.root,self.raw)\n",
    "        processed_path = os.path.join(self.root,self.processed)\n",
    "\n",
    "        print(\"Processing training data\")\n",
    "        train_image = self.readimg(self.root, self.file_name[0], 2051)\n",
    "        train_label\t= self.readlab(self.root, self.file_name[1], 2049)\n",
    "        train_data = (train_image,train_label)\n",
    "\n",
    "        print(\"Processing testing data\")\n",
    "        test_image = self.readimg(self.root, self.file_name[2], 2051)\n",
    "        test_label = self.readlab(self.root, self.file_name[3], 2049)\n",
    "        test_data = (test_image,test_label)\n",
    "\n",
    "        train_path = os.path.join(self.root,self.processed,\"training_set.pt\")\n",
    "        with open(train_path,\"wb\") as f:\n",
    "            torch.save(train_data,f)\n",
    "\n",
    "        test_path = os.path.join(self.root,self.processed,\"testing_set.pt\")\n",
    "        with open(test_path,\"wb\") as f:\n",
    "            torch.save(test_data,f)\n",
    "        print()\n",
    "        print(\"Processed data has been stored in your specified root directory under /processsed\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    def readimg(self, root, file, magic):\n",
    "        image = []\n",
    "        path = os.path.join(self.root,self.raw,file)\n",
    "        with open(path,'rb') as f:\n",
    "            magic_number, size, row, col = struct.unpack('>IIII',f.read(16))\n",
    "            assert (magic_number == magic)\n",
    "            for run in range(size*row*col):\n",
    "                image.append(list(struct.unpack('B',f.read(1)))[0])\n",
    "            image = np.asarray(image, dtype = np.float32)\n",
    "            return (torch.from_numpy(image).view(size,1,row,col))\n",
    "\n",
    "\n",
    "    def readlab(self, root, file, magic):\n",
    "        label = []\n",
    "        path = os.path.join(self.root,self.raw,file)\n",
    "        with open(path,'rb') as f:\n",
    "            magic_number, size = struct.unpack(\">II\",f.read(8))\n",
    "            assert (magic_number == magic)\n",
    "            for run in range(size):\n",
    "                label.append(list(struct.unpack('b',f.read(1)))[0])\n",
    "            label = np.asarray(label)\n",
    "            return (torch.from_numpy(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnH73n-isy4K",
    "outputId": "772cd63a-3e6e-4e52-cf9e-3ce5dcec8ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Extracting: train-images-idx3-ubyte.gz\n",
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Extracting: train-labels-idx1-ubyte.gz\n",
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Extracting: t10k-images-idx3-ubyte.gz\n",
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Extracting: t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Raw data downloaded and extracted in your specified root directory under /raw\n",
      "\n",
      "Processing training data\n",
      "Processing testing data\n",
      "\n",
      "Processed data has been stored in your specified root directory under /processsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extract the training and testing dataset in a folder named Fashion MNIS\n",
    "train_dataset = fmnist(root = \".\\FashionMNIS\", train = True, transform = True, download = True)\n",
    "test_dataset = fmnist(root = \".\\FashionMNIS\", train = False, transform = False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PzQggj61s-ML"
   },
   "outputs": [],
   "source": [
    "#Calculation of total epochs using a defined batch size(batch_size)\n",
    "#and total iterations(n_ters)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 18000\n",
    "epoch_size = n_iters/(len(train_dataset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mWaTbHfmtA-T"
   },
   "outputs": [],
   "source": [
    "#Convolutional Networks\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (CNNModel, self).__init__()\n",
    "\n",
    "#Convolution 1  with kernel size 3\n",
    "        self.cnn1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "#Batch Normalization\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        nn.init.xavier_uniform(self.cnn1.weight)\n",
    "\n",
    "#Max pooling for reducing dimensions\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "#Second convolution to reduce and sharpen the image further\n",
    "        self.cnn2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "#Batch Normalization\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        nn.init.xavier_uniform(self.cnn2.weight)\n",
    "\n",
    "#Max Pooling again for reduction of pixels\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 4096)\n",
    "        self.fcrelu = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.norm1(out)\n",
    "\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        out = out.view(out.size(0),-1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.fcrelu(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f7XbX4FtD_R",
    "outputId": "8a0e0a74-7733-4865-ee2c-96af86ce3a40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-aa0e945d6b5e>:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.cnn1.weight)\n",
      "<ipython-input-5-aa0e945d6b5e>:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.cnn2.weight)\n"
     ]
    }
   ],
   "source": [
    "#Load the model\n",
    "model = CNNModel()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dDJoic5ntGmc"
   },
   "outputs": [],
   "source": [
    "#I have used Cross Entropy for Categorical Predictions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.015\n",
    "moment = 0.9\n",
    "optimizer = optim.SGD(model.parameters(),lr = learning_rate, momentum = moment, nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muRXpt3TtIuY",
    "outputId": "08290953-ef87-446d-b2e6-b29e46f24411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3000 Loss: 0.19170606136322021 Accuracy: 92.29999542236328\n",
      "Iteration: 6000 Loss: 0.07875138521194458 Accuracy: 92.33999633789062\n",
      "Iteration: 9000 Loss: 0.03824256360530853 Accuracy: 92.51000213623047\n",
      "Iteration: 12000 Loss: 0.002043476328253746 Accuracy: 93.0199966430664\n",
      "Iteration: 15000 Loss: 0.00025225349236279726 Accuracy: 93.41999816894531\n",
      "Iteration: 18000 Loss: 5.1569280913099647e-05 Accuracy: 93.51000213623047\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iter = 0\n",
    "for epoch in range(int(math.ceil(epoch_size))):\n",
    "\n",
    "    '''\n",
    "    Loading the training dataset after every epoch which will\n",
    "    load it from the Fashion Dataset Class making a new train\n",
    "    loader for every new epoch causing random flips and random\n",
    "    shuffling of above exampled Fashion MNIST images.\n",
    "    '''\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = True)\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        labels=labels.long()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        iter += 1\n",
    "\n",
    "#At every 3000th epoch a test on the above initialised test dataset\n",
    "#(test_loader) would be performed and an accuracy would be provided.\n",
    "\n",
    "\n",
    "        if iter%3000 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for image,label in test_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    image = Variable(image.cuda())\n",
    "                else:\n",
    "                    image = Variable(image)\n",
    "                output = model(image)\n",
    "                _, predicted = torch.max(output.data,1)\n",
    "                total += label.size(0)\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == label.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == label).sum()\n",
    "\n",
    "            accuracy = 100 * (correct/total)\n",
    "            print('Iteration: {} Loss: {} Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fqrDaQ837cAu"
   },
   "outputs": [],
   "source": [
    "#Saving Model at the current trained state\n",
    "PATH= \"model1.pt\"\n",
    "torch.save({\n",
    "            'epoch': epoch_size,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': criterion\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FcPa-hw9EF3",
    "outputId": "1bb3507e-4cf0-4516-bd33-29c8b9ece4ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-aa0e945d6b5e>:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.cnn1.weight)\n",
      "<ipython-input-5-aa0e945d6b5e>:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.cnn2.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0183,  0.0223, -0.0603],\n",
       "          [-0.0250, -0.0659, -0.0413],\n",
       "          [-0.0332, -0.0053, -0.0545]],\n",
       "\n",
       "         [[-0.0327,  0.0412,  0.0217],\n",
       "          [ 0.0748, -0.0031,  0.0418],\n",
       "          [ 0.0595, -0.0034, -0.1220]],\n",
       "\n",
       "         [[-0.0628,  0.0176, -0.0930],\n",
       "          [-0.0778, -0.0346,  0.0551],\n",
       "          [-0.1073, -0.0338,  0.0139]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0463,  0.0374,  0.0067],\n",
       "          [ 0.0590,  0.0569,  0.0302],\n",
       "          [-0.0045,  0.0022,  0.0411]],\n",
       "\n",
       "         [[-0.1131, -0.0489, -0.0631],\n",
       "          [ 0.0268, -0.0382, -0.0685],\n",
       "          [ 0.0251, -0.1256,  0.0401]],\n",
       "\n",
       "         [[ 0.0395,  0.1339, -0.0135],\n",
       "          [ 0.0434,  0.1039,  0.0779],\n",
       "          [ 0.0393, -0.0955, -0.0250]]],\n",
       "\n",
       "\n",
       "        [[[-0.0970,  0.0368,  0.0346],\n",
       "          [ 0.0750,  0.0374, -0.0094],\n",
       "          [-0.0085,  0.0495, -0.0008]],\n",
       "\n",
       "         [[ 0.0033, -0.0360, -0.0971],\n",
       "          [-0.0382,  0.0776,  0.0135],\n",
       "          [ 0.0976, -0.0691,  0.0532]],\n",
       "\n",
       "         [[ 0.0367,  0.0084, -0.0671],\n",
       "          [ 0.0086, -0.0184,  0.0196],\n",
       "          [ 0.0023,  0.0542,  0.0959]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0371,  0.0365, -0.0337],\n",
       "          [-0.0381,  0.0995,  0.0105],\n",
       "          [ 0.0287,  0.0150, -0.0091]],\n",
       "\n",
       "         [[-0.0258, -0.0120, -0.1363],\n",
       "          [ 0.0116, -0.0851, -0.1110],\n",
       "          [-0.0918,  0.0761,  0.0194]],\n",
       "\n",
       "         [[-0.0166,  0.0210, -0.0474],\n",
       "          [-0.0334, -0.0398,  0.0631],\n",
       "          [ 0.0635,  0.0607,  0.0051]]],\n",
       "\n",
       "\n",
       "        [[[-0.0349, -0.1008, -0.0495],\n",
       "          [-0.0025, -0.0700,  0.0564],\n",
       "          [-0.0094,  0.0253,  0.0635]],\n",
       "\n",
       "         [[-0.0790, -0.0096,  0.0378],\n",
       "          [ 0.0102, -0.0390,  0.0574],\n",
       "          [-0.0048,  0.0812, -0.0374]],\n",
       "\n",
       "         [[-0.0490, -0.0969, -0.0446],\n",
       "          [ 0.0044, -0.1145,  0.0080],\n",
       "          [ 0.0829, -0.0159, -0.0729]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0391, -0.0825,  0.0007],\n",
       "          [-0.0009, -0.0812, -0.0455],\n",
       "          [ 0.0231,  0.0165, -0.0408]],\n",
       "\n",
       "         [[ 0.0299, -0.0087, -0.0757],\n",
       "          [ 0.0239,  0.0589, -0.0079],\n",
       "          [ 0.0316, -0.0815, -0.0056]],\n",
       "\n",
       "         [[-0.0348,  0.0154, -0.0741],\n",
       "          [-0.0547,  0.0275, -0.0815],\n",
       "          [ 0.1103, -0.0865,  0.0627]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.0454,  0.0302, -0.0674],\n",
       "          [ 0.0511, -0.0263,  0.0235],\n",
       "          [ 0.0017, -0.0487, -0.0460]],\n",
       "\n",
       "         [[-0.0633, -0.0413,  0.0523],\n",
       "          [ 0.0772, -0.0961, -0.0191],\n",
       "          [-0.0567, -0.1134,  0.0244]],\n",
       "\n",
       "         [[-0.0939, -0.1438, -0.0735],\n",
       "          [ 0.0266, -0.1404, -0.0087],\n",
       "          [-0.1263, -0.0739,  0.0583]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0012, -0.0454,  0.0707],\n",
       "          [-0.0509, -0.0406, -0.0623],\n",
       "          [ 0.0678, -0.0176, -0.0973]],\n",
       "\n",
       "         [[-0.0669, -0.0593, -0.0247],\n",
       "          [ 0.0564,  0.0669, -0.0663],\n",
       "          [-0.0692,  0.0151, -0.0319]],\n",
       "\n",
       "         [[-0.1320,  0.0549, -0.1106],\n",
       "          [ 0.1333, -0.0133, -0.0691],\n",
       "          [ 0.1063, -0.0234, -0.0269]]],\n",
       "\n",
       "\n",
       "        [[[-0.0020, -0.0800, -0.0341],\n",
       "          [ 0.0706, -0.0091, -0.0214],\n",
       "          [ 0.1016,  0.0219, -0.1058]],\n",
       "\n",
       "         [[-0.0602, -0.0026, -0.0496],\n",
       "          [ 0.0144, -0.0449, -0.0271],\n",
       "          [-0.0380, -0.0739, -0.0475]],\n",
       "\n",
       "         [[ 0.0016,  0.0625, -0.1683],\n",
       "          [ 0.0784, -0.0391, -0.0288],\n",
       "          [ 0.0326, -0.1322, -0.0025]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0432, -0.0174, -0.0313],\n",
       "          [ 0.0682,  0.0385,  0.0772],\n",
       "          [-0.0636,  0.0062,  0.0554]],\n",
       "\n",
       "         [[-0.0040,  0.0307, -0.0561],\n",
       "          [-0.0451,  0.0789, -0.0529],\n",
       "          [ 0.0048, -0.0758,  0.0371]],\n",
       "\n",
       "         [[-0.0251,  0.0576, -0.1012],\n",
       "          [-0.0763,  0.1419, -0.0182],\n",
       "          [ 0.0172, -0.0705, -0.1589]]],\n",
       "\n",
       "\n",
       "        [[[-0.1101, -0.0978, -0.0363],\n",
       "          [ 0.0382,  0.0144,  0.0065],\n",
       "          [-0.0178,  0.0101, -0.0769]],\n",
       "\n",
       "         [[-0.0408,  0.0187, -0.0746],\n",
       "          [-0.0664,  0.0300, -0.0229],\n",
       "          [ 0.0614,  0.0005,  0.0912]],\n",
       "\n",
       "         [[ 0.0396,  0.0653, -0.0403],\n",
       "          [-0.0631, -0.0410, -0.0468],\n",
       "          [ 0.0761, -0.0160,  0.0124]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0094,  0.0364,  0.0374],\n",
       "          [ 0.0383, -0.0360, -0.0670],\n",
       "          [-0.0594,  0.0493, -0.0015]],\n",
       "\n",
       "         [[ 0.0015, -0.0785,  0.0583],\n",
       "          [-0.0713, -0.0857,  0.0274],\n",
       "          [ 0.0030, -0.0555, -0.0634]],\n",
       "\n",
       "         [[-0.0710,  0.0406, -0.1072],\n",
       "          [-0.0826, -0.0029,  0.0767],\n",
       "          [-0.0180, -0.1178, -0.0652]]]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SCRIPT for opening the model again\n",
    "\n",
    "PATH= \"model1.pt\"\n",
    "model1 = CNNModel()\n",
    "optimizer1 = optim.SGD(model.parameters(),lr = 0.015, momentum = 0.9, nesterov = True)\n",
    "checkpoint = torch.load(PATH)\n",
    "\n",
    "#Saving Model Weights\n",
    "torch.save(model1.load_state_dict(checkpoint['model_state_dict']),'model1_weights.pth')\n",
    "optimizer1.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch1 = checkpoint['epoch']\n",
    "loss1 = checkpoint['loss']\n",
    "model1.eval()\n",
    "model1.cnn1.weight\n",
    "model1.cnn2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXpsebhKKfEV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
